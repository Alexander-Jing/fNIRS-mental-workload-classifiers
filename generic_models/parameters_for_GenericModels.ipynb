{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "from tqdm import trange\n",
    "from torchsummary import summary\n",
    "from torchstat import stat\n",
    "from thop import profile\n",
    "\n",
    "YOUR_PATH = \"/home/jyt/workspace/fNIRS_models/code_data_tufts\"\n",
    "sys.path.insert(0, YOUR_PATH + '/fNIRS-mental-workload-classifiers/helpers')\n",
    "import models\n",
    "import brain_data\n",
    "from utils import generic_GetTrainValTestSubjects, seed_everything, makedir_if_not_exist, plot_confusion_matrix, save_pickle, train_one_epoch, eval_model, save_training_curves_FixedTrainValSplit, write_performance_info_FixedTrainValSplit, write_program_time, write_inference_time\n",
    "from utils import LabelSmoothing, train_one_epoch_fNIRS_T, eval_model_fNIRST, train_one_epoch_Ours_T, eval_model_OursT\n",
    "from utils import EarlyStopping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OursT parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected GPUs\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print('Detected GPUs', flush = True)\n",
    "    #device = torch.device('cuda')\n",
    "    device = torch.device('cuda:{}'.format(0))\n",
    "else:\n",
    "    print('DID NOT detect GPUs', flush = True)\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_use = models.Ours_T\n",
    "model = model_to_use(n_class=2, sampling_points=150, patch_length=30, dim=64, depth=6, heads=8, mlp_dim=256).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Rearrange-1                [1, 8, 150]               0\n",
      "         Rearrange-2              [1, 8, 5, 30]               0\n",
      "         Rearrange-3              [1, 5, 8, 30]               0\n",
      "         Rearrange-4                [1, 5, 240]               0\n",
      "            Linear-5                 [1, 5, 64]          15,424\n",
      "           Dropout-6                 [1, 6, 64]               0\n",
      "         LayerNorm-7                 [1, 6, 64]             128\n",
      "            Linear-8               [1, 6, 1536]          98,304\n",
      "            Linear-9                 [1, 6, 64]          32,832\n",
      "          Dropout-10                 [1, 6, 64]               0\n",
      "        Attention-11                 [1, 6, 64]               0\n",
      "          PreNorm-12                 [1, 6, 64]               0\n",
      "         Residual-13                 [1, 6, 64]               0\n",
      "        LayerNorm-14                 [1, 6, 64]             128\n",
      "           Linear-15                [1, 6, 256]          16,640\n",
      "             GELU-16                [1, 6, 256]               0\n",
      "          Dropout-17                [1, 6, 256]               0\n",
      "           Linear-18                 [1, 6, 64]          16,448\n",
      "          Dropout-19                 [1, 6, 64]               0\n",
      "      FeedForward-20                 [1, 6, 64]               0\n",
      "          PreNorm-21                 [1, 6, 64]               0\n",
      "         Residual-22                 [1, 6, 64]               0\n",
      "        LayerNorm-23                 [1, 6, 64]             128\n",
      "           Linear-24               [1, 6, 1536]          98,304\n",
      "           Linear-25                 [1, 6, 64]          32,832\n",
      "          Dropout-26                 [1, 6, 64]               0\n",
      "        Attention-27                 [1, 6, 64]               0\n",
      "          PreNorm-28                 [1, 6, 64]               0\n",
      "         Residual-29                 [1, 6, 64]               0\n",
      "        LayerNorm-30                 [1, 6, 64]             128\n",
      "           Linear-31                [1, 6, 256]          16,640\n",
      "             GELU-32                [1, 6, 256]               0\n",
      "          Dropout-33                [1, 6, 256]               0\n",
      "           Linear-34                 [1, 6, 64]          16,448\n",
      "          Dropout-35                 [1, 6, 64]               0\n",
      "      FeedForward-36                 [1, 6, 64]               0\n",
      "          PreNorm-37                 [1, 6, 64]               0\n",
      "         Residual-38                 [1, 6, 64]               0\n",
      "        LayerNorm-39                 [1, 6, 64]             128\n",
      "           Linear-40               [1, 6, 1536]          98,304\n",
      "           Linear-41                 [1, 6, 64]          32,832\n",
      "          Dropout-42                 [1, 6, 64]               0\n",
      "        Attention-43                 [1, 6, 64]               0\n",
      "          PreNorm-44                 [1, 6, 64]               0\n",
      "         Residual-45                 [1, 6, 64]               0\n",
      "        LayerNorm-46                 [1, 6, 64]             128\n",
      "           Linear-47                [1, 6, 256]          16,640\n",
      "             GELU-48                [1, 6, 256]               0\n",
      "          Dropout-49                [1, 6, 256]               0\n",
      "           Linear-50                 [1, 6, 64]          16,448\n",
      "          Dropout-51                 [1, 6, 64]               0\n",
      "      FeedForward-52                 [1, 6, 64]               0\n",
      "          PreNorm-53                 [1, 6, 64]               0\n",
      "         Residual-54                 [1, 6, 64]               0\n",
      "        LayerNorm-55                 [1, 6, 64]             128\n",
      "           Linear-56               [1, 6, 1536]          98,304\n",
      "           Linear-57                 [1, 6, 64]          32,832\n",
      "          Dropout-58                 [1, 6, 64]               0\n",
      "        Attention-59                 [1, 6, 64]               0\n",
      "          PreNorm-60                 [1, 6, 64]               0\n",
      "         Residual-61                 [1, 6, 64]               0\n",
      "        LayerNorm-62                 [1, 6, 64]             128\n",
      "           Linear-63                [1, 6, 256]          16,640\n",
      "             GELU-64                [1, 6, 256]               0\n",
      "          Dropout-65                [1, 6, 256]               0\n",
      "           Linear-66                 [1, 6, 64]          16,448\n",
      "          Dropout-67                 [1, 6, 64]               0\n",
      "      FeedForward-68                 [1, 6, 64]               0\n",
      "          PreNorm-69                 [1, 6, 64]               0\n",
      "         Residual-70                 [1, 6, 64]               0\n",
      "        LayerNorm-71                 [1, 6, 64]             128\n",
      "           Linear-72               [1, 6, 1536]          98,304\n",
      "           Linear-73                 [1, 6, 64]          32,832\n",
      "          Dropout-74                 [1, 6, 64]               0\n",
      "        Attention-75                 [1, 6, 64]               0\n",
      "          PreNorm-76                 [1, 6, 64]               0\n",
      "         Residual-77                 [1, 6, 64]               0\n",
      "        LayerNorm-78                 [1, 6, 64]             128\n",
      "           Linear-79                [1, 6, 256]          16,640\n",
      "             GELU-80                [1, 6, 256]               0\n",
      "          Dropout-81                [1, 6, 256]               0\n",
      "           Linear-82                 [1, 6, 64]          16,448\n",
      "          Dropout-83                 [1, 6, 64]               0\n",
      "      FeedForward-84                 [1, 6, 64]               0\n",
      "          PreNorm-85                 [1, 6, 64]               0\n",
      "         Residual-86                 [1, 6, 64]               0\n",
      "        LayerNorm-87                 [1, 6, 64]             128\n",
      "           Linear-88               [1, 6, 1536]          98,304\n",
      "           Linear-89                 [1, 6, 64]          32,832\n",
      "          Dropout-90                 [1, 6, 64]               0\n",
      "        Attention-91                 [1, 6, 64]               0\n",
      "          PreNorm-92                 [1, 6, 64]               0\n",
      "         Residual-93                 [1, 6, 64]               0\n",
      "        LayerNorm-94                 [1, 6, 64]             128\n",
      "           Linear-95                [1, 6, 256]          16,640\n",
      "             GELU-96                [1, 6, 256]               0\n",
      "          Dropout-97                [1, 6, 256]               0\n",
      "           Linear-98                 [1, 6, 64]          16,448\n",
      "          Dropout-99                 [1, 6, 64]               0\n",
      "     FeedForward-100                 [1, 6, 64]               0\n",
      "         PreNorm-101                 [1, 6, 64]               0\n",
      "        Residual-102                 [1, 6, 64]               0\n",
      "     Transformer-103                 [1, 6, 64]               0\n",
      "       LayerNorm-104                    [1, 64]             128\n",
      "          Linear-105                     [1, 2]             130\n",
      "================================================================\n",
      "Total params: 1,002,562\n",
      "Trainable params: 1,002,562\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.89\n",
      "Params size (MB): 3.82\n",
      "Estimated Total Size (MB): 4.72\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(150, 8), batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "flops:  23975424.0 params:  1002562.0\n",
      "flops: 23.98 M, params: 1.00 M\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(2, 150, 8).to(device)\n",
    "flops, params = profile(model,(dummy_input,))\n",
    "print('flops: ', flops, 'params: ', params)\n",
    "print('flops: %.2f M, params: %.2f M' % (flops / 1000000.0, params / 1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_use_1 = models.fNIRS_PreT\n",
    "model_1 = model_to_use_1(n_class=2, sampling_point=300, dim=64, depth=6, heads=8, mlp_dim=64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         AvgPool1d-1                [2, 2, 300]               0\n",
      "         AvgPool1d-2                [2, 2, 300]               0\n",
      "         AvgPool1d-3                [2, 2, 300]               0\n",
      "         LayerNorm-4                [2, 2, 300]             600\n",
      "         AvgPool1d-5                [2, 2, 300]               0\n",
      "         AvgPool1d-6                [2, 2, 300]               0\n",
      "         AvgPool1d-7                [2, 2, 300]               0\n",
      "         LayerNorm-8                [2, 2, 300]             600\n",
      "          PreBlock-9             [2, 2, 2, 300]               0\n",
      "           Conv2d-10              [2, 8, 3, 91]             968\n",
      "        Rearrange-11                [2, 3, 728]               0\n",
      "           Linear-12                 [2, 3, 64]          46,656\n",
      "        LayerNorm-13                 [2, 3, 64]             128\n",
      "           Conv2d-14              [2, 8, 2, 91]             488\n",
      "        Rearrange-15                [2, 2, 728]               0\n",
      "           Linear-16                 [2, 2, 64]          46,656\n",
      "        LayerNorm-17                 [2, 2, 64]             128\n",
      "          Dropout-18                 [2, 4, 64]               0\n",
      "        LayerNorm-19                 [2, 4, 64]             128\n",
      "           Linear-20               [2, 4, 1536]          98,304\n",
      "           Linear-21                 [2, 4, 64]          32,832\n",
      "          Dropout-22                 [2, 4, 64]               0\n",
      "        Attention-23                 [2, 4, 64]               0\n",
      "          PreNorm-24                 [2, 4, 64]               0\n",
      "         Residual-25                 [2, 4, 64]               0\n",
      "        LayerNorm-26                 [2, 4, 64]             128\n",
      "           Linear-27                 [2, 4, 64]           4,160\n",
      "             GELU-28                 [2, 4, 64]               0\n",
      "          Dropout-29                 [2, 4, 64]               0\n",
      "           Linear-30                 [2, 4, 64]           4,160\n",
      "          Dropout-31                 [2, 4, 64]               0\n",
      "      FeedForward-32                 [2, 4, 64]               0\n",
      "          PreNorm-33                 [2, 4, 64]               0\n",
      "         Residual-34                 [2, 4, 64]               0\n",
      "        LayerNorm-35                 [2, 4, 64]             128\n",
      "           Linear-36               [2, 4, 1536]          98,304\n",
      "           Linear-37                 [2, 4, 64]          32,832\n",
      "          Dropout-38                 [2, 4, 64]               0\n",
      "        Attention-39                 [2, 4, 64]               0\n",
      "          PreNorm-40                 [2, 4, 64]               0\n",
      "         Residual-41                 [2, 4, 64]               0\n",
      "        LayerNorm-42                 [2, 4, 64]             128\n",
      "           Linear-43                 [2, 4, 64]           4,160\n",
      "             GELU-44                 [2, 4, 64]               0\n",
      "          Dropout-45                 [2, 4, 64]               0\n",
      "           Linear-46                 [2, 4, 64]           4,160\n",
      "          Dropout-47                 [2, 4, 64]               0\n",
      "      FeedForward-48                 [2, 4, 64]               0\n",
      "          PreNorm-49                 [2, 4, 64]               0\n",
      "         Residual-50                 [2, 4, 64]               0\n",
      "        LayerNorm-51                 [2, 4, 64]             128\n",
      "           Linear-52               [2, 4, 1536]          98,304\n",
      "           Linear-53                 [2, 4, 64]          32,832\n",
      "          Dropout-54                 [2, 4, 64]               0\n",
      "        Attention-55                 [2, 4, 64]               0\n",
      "          PreNorm-56                 [2, 4, 64]               0\n",
      "         Residual-57                 [2, 4, 64]               0\n",
      "        LayerNorm-58                 [2, 4, 64]             128\n",
      "           Linear-59                 [2, 4, 64]           4,160\n",
      "             GELU-60                 [2, 4, 64]               0\n",
      "          Dropout-61                 [2, 4, 64]               0\n",
      "           Linear-62                 [2, 4, 64]           4,160\n",
      "          Dropout-63                 [2, 4, 64]               0\n",
      "      FeedForward-64                 [2, 4, 64]               0\n",
      "          PreNorm-65                 [2, 4, 64]               0\n",
      "         Residual-66                 [2, 4, 64]               0\n",
      "        LayerNorm-67                 [2, 4, 64]             128\n",
      "           Linear-68               [2, 4, 1536]          98,304\n",
      "           Linear-69                 [2, 4, 64]          32,832\n",
      "          Dropout-70                 [2, 4, 64]               0\n",
      "        Attention-71                 [2, 4, 64]               0\n",
      "          PreNorm-72                 [2, 4, 64]               0\n",
      "         Residual-73                 [2, 4, 64]               0\n",
      "        LayerNorm-74                 [2, 4, 64]             128\n",
      "           Linear-75                 [2, 4, 64]           4,160\n",
      "             GELU-76                 [2, 4, 64]               0\n",
      "          Dropout-77                 [2, 4, 64]               0\n",
      "           Linear-78                 [2, 4, 64]           4,160\n",
      "          Dropout-79                 [2, 4, 64]               0\n",
      "      FeedForward-80                 [2, 4, 64]               0\n",
      "          PreNorm-81                 [2, 4, 64]               0\n",
      "         Residual-82                 [2, 4, 64]               0\n",
      "        LayerNorm-83                 [2, 4, 64]             128\n",
      "           Linear-84               [2, 4, 1536]          98,304\n",
      "           Linear-85                 [2, 4, 64]          32,832\n",
      "          Dropout-86                 [2, 4, 64]               0\n",
      "        Attention-87                 [2, 4, 64]               0\n",
      "          PreNorm-88                 [2, 4, 64]               0\n",
      "         Residual-89                 [2, 4, 64]               0\n",
      "        LayerNorm-90                 [2, 4, 64]             128\n",
      "           Linear-91                 [2, 4, 64]           4,160\n",
      "             GELU-92                 [2, 4, 64]               0\n",
      "          Dropout-93                 [2, 4, 64]               0\n",
      "           Linear-94                 [2, 4, 64]           4,160\n",
      "          Dropout-95                 [2, 4, 64]               0\n",
      "      FeedForward-96                 [2, 4, 64]               0\n",
      "          PreNorm-97                 [2, 4, 64]               0\n",
      "         Residual-98                 [2, 4, 64]               0\n",
      "        LayerNorm-99                 [2, 4, 64]             128\n",
      "          Linear-100               [2, 4, 1536]          98,304\n",
      "          Linear-101                 [2, 4, 64]          32,832\n",
      "         Dropout-102                 [2, 4, 64]               0\n",
      "       Attention-103                 [2, 4, 64]               0\n",
      "         PreNorm-104                 [2, 4, 64]               0\n",
      "        Residual-105                 [2, 4, 64]               0\n",
      "       LayerNorm-106                 [2, 4, 64]             128\n",
      "          Linear-107                 [2, 4, 64]           4,160\n",
      "            GELU-108                 [2, 4, 64]               0\n",
      "         Dropout-109                 [2, 4, 64]               0\n",
      "          Linear-110                 [2, 4, 64]           4,160\n",
      "         Dropout-111                 [2, 4, 64]               0\n",
      "     FeedForward-112                 [2, 4, 64]               0\n",
      "         PreNorm-113                 [2, 4, 64]               0\n",
      "        Residual-114                 [2, 4, 64]               0\n",
      "     Transformer-115                 [2, 4, 64]               0\n",
      "         Dropout-116                 [2, 3, 64]               0\n",
      "       LayerNorm-117                 [2, 3, 64]             128\n",
      "          Linear-118               [2, 3, 1536]          98,304\n",
      "          Linear-119                 [2, 3, 64]          32,832\n",
      "         Dropout-120                 [2, 3, 64]               0\n",
      "       Attention-121                 [2, 3, 64]               0\n",
      "         PreNorm-122                 [2, 3, 64]               0\n",
      "        Residual-123                 [2, 3, 64]               0\n",
      "       LayerNorm-124                 [2, 3, 64]             128\n",
      "          Linear-125                 [2, 3, 64]           4,160\n",
      "            GELU-126                 [2, 3, 64]               0\n",
      "         Dropout-127                 [2, 3, 64]               0\n",
      "          Linear-128                 [2, 3, 64]           4,160\n",
      "         Dropout-129                 [2, 3, 64]               0\n",
      "     FeedForward-130                 [2, 3, 64]               0\n",
      "         PreNorm-131                 [2, 3, 64]               0\n",
      "        Residual-132                 [2, 3, 64]               0\n",
      "       LayerNorm-133                 [2, 3, 64]             128\n",
      "          Linear-134               [2, 3, 1536]          98,304\n",
      "          Linear-135                 [2, 3, 64]          32,832\n",
      "         Dropout-136                 [2, 3, 64]               0\n",
      "       Attention-137                 [2, 3, 64]               0\n",
      "         PreNorm-138                 [2, 3, 64]               0\n",
      "        Residual-139                 [2, 3, 64]               0\n",
      "       LayerNorm-140                 [2, 3, 64]             128\n",
      "          Linear-141                 [2, 3, 64]           4,160\n",
      "            GELU-142                 [2, 3, 64]               0\n",
      "         Dropout-143                 [2, 3, 64]               0\n",
      "          Linear-144                 [2, 3, 64]           4,160\n",
      "         Dropout-145                 [2, 3, 64]               0\n",
      "     FeedForward-146                 [2, 3, 64]               0\n",
      "         PreNorm-147                 [2, 3, 64]               0\n",
      "        Residual-148                 [2, 3, 64]               0\n",
      "       LayerNorm-149                 [2, 3, 64]             128\n",
      "          Linear-150               [2, 3, 1536]          98,304\n",
      "          Linear-151                 [2, 3, 64]          32,832\n",
      "         Dropout-152                 [2, 3, 64]               0\n",
      "       Attention-153                 [2, 3, 64]               0\n",
      "         PreNorm-154                 [2, 3, 64]               0\n",
      "        Residual-155                 [2, 3, 64]               0\n",
      "       LayerNorm-156                 [2, 3, 64]             128\n",
      "          Linear-157                 [2, 3, 64]           4,160\n",
      "            GELU-158                 [2, 3, 64]               0\n",
      "         Dropout-159                 [2, 3, 64]               0\n",
      "          Linear-160                 [2, 3, 64]           4,160\n",
      "         Dropout-161                 [2, 3, 64]               0\n",
      "     FeedForward-162                 [2, 3, 64]               0\n",
      "         PreNorm-163                 [2, 3, 64]               0\n",
      "        Residual-164                 [2, 3, 64]               0\n",
      "       LayerNorm-165                 [2, 3, 64]             128\n",
      "          Linear-166               [2, 3, 1536]          98,304\n",
      "          Linear-167                 [2, 3, 64]          32,832\n",
      "         Dropout-168                 [2, 3, 64]               0\n",
      "       Attention-169                 [2, 3, 64]               0\n",
      "         PreNorm-170                 [2, 3, 64]               0\n",
      "        Residual-171                 [2, 3, 64]               0\n",
      "       LayerNorm-172                 [2, 3, 64]             128\n",
      "          Linear-173                 [2, 3, 64]           4,160\n",
      "            GELU-174                 [2, 3, 64]               0\n",
      "         Dropout-175                 [2, 3, 64]               0\n",
      "          Linear-176                 [2, 3, 64]           4,160\n",
      "         Dropout-177                 [2, 3, 64]               0\n",
      "     FeedForward-178                 [2, 3, 64]               0\n",
      "         PreNorm-179                 [2, 3, 64]               0\n",
      "        Residual-180                 [2, 3, 64]               0\n",
      "       LayerNorm-181                 [2, 3, 64]             128\n",
      "          Linear-182               [2, 3, 1536]          98,304\n",
      "          Linear-183                 [2, 3, 64]          32,832\n",
      "         Dropout-184                 [2, 3, 64]               0\n",
      "       Attention-185                 [2, 3, 64]               0\n",
      "         PreNorm-186                 [2, 3, 64]               0\n",
      "        Residual-187                 [2, 3, 64]               0\n",
      "       LayerNorm-188                 [2, 3, 64]             128\n",
      "          Linear-189                 [2, 3, 64]           4,160\n",
      "            GELU-190                 [2, 3, 64]               0\n",
      "         Dropout-191                 [2, 3, 64]               0\n",
      "          Linear-192                 [2, 3, 64]           4,160\n",
      "         Dropout-193                 [2, 3, 64]               0\n",
      "     FeedForward-194                 [2, 3, 64]               0\n",
      "         PreNorm-195                 [2, 3, 64]               0\n",
      "        Residual-196                 [2, 3, 64]               0\n",
      "       LayerNorm-197                 [2, 3, 64]             128\n",
      "          Linear-198               [2, 3, 1536]          98,304\n",
      "          Linear-199                 [2, 3, 64]          32,832\n",
      "         Dropout-200                 [2, 3, 64]               0\n",
      "       Attention-201                 [2, 3, 64]               0\n",
      "         PreNorm-202                 [2, 3, 64]               0\n",
      "        Residual-203                 [2, 3, 64]               0\n",
      "       LayerNorm-204                 [2, 3, 64]             128\n",
      "          Linear-205                 [2, 3, 64]           4,160\n",
      "            GELU-206                 [2, 3, 64]               0\n",
      "         Dropout-207                 [2, 3, 64]               0\n",
      "          Linear-208                 [2, 3, 64]           4,160\n",
      "         Dropout-209                 [2, 3, 64]               0\n",
      "     FeedForward-210                 [2, 3, 64]               0\n",
      "         PreNorm-211                 [2, 3, 64]               0\n",
      "        Residual-212                 [2, 3, 64]               0\n",
      "     Transformer-213                 [2, 3, 64]               0\n",
      "        Identity-214                    [2, 64]               0\n",
      "        Identity-215                    [2, 64]               0\n",
      "       LayerNorm-216                   [2, 128]             256\n",
      "          Linear-217                     [2, 2]             258\n",
      "         fNIRS_T-218                     [2, 2]               0\n",
      "================================================================\n",
      "Total params: 1,773,282\n",
      "Trainable params: 1,773,282\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.83\n",
      "Params size (MB): 6.76\n",
      "Estimated Total Size (MB): 8.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model_1, input_size=(2,2,300), batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_avgpool() for <class 'torch.nn.modules.pooling.AvgPool1d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "flops:  51707520.0 params:  1773282.0\n",
      "flops: 51.71 M, params: 1.77 M\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(2, 2, 2, 300).to(device)\n",
    "flops, params = profile(model_1,(dummy_input,))\n",
    "print('flops: ', flops, 'params: ', params)\n",
    "print('flops: %.2f M, params: %.2f M' % (flops / 1000000.0, params / 1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_use_2 = models.DCNN\n",
    "model_2 = model_to_use_2(n_class=2,dropout=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Rearrange-1                [1, 8, 150]               0\n",
      "            Conv2d-2            [1, 25, 8, 146]             150\n",
      "            Conv2d-3            [1, 25, 1, 146]           5,025\n",
      "       BatchNorm2d-4            [1, 25, 1, 146]              50\n",
      "               ELU-5            [1, 25, 1, 146]               0\n",
      "         MaxPool2d-6             [1, 25, 1, 73]               0\n",
      "           Dropout-7             [1, 25, 1, 73]               0\n",
      "            Conv2d-8             [1, 50, 1, 69]           6,300\n",
      "       BatchNorm2d-9             [1, 50, 1, 69]             100\n",
      "              ELU-10             [1, 50, 1, 69]               0\n",
      "        MaxPool2d-11             [1, 50, 1, 35]               0\n",
      "          Dropout-12             [1, 50, 1, 35]               0\n",
      "           Conv2d-13            [1, 100, 1, 31]          25,100\n",
      "      BatchNorm2d-14            [1, 100, 1, 31]             200\n",
      "              ELU-15            [1, 100, 1, 31]               0\n",
      "        MaxPool2d-16            [1, 100, 1, 16]               0\n",
      "          Dropout-17            [1, 100, 1, 16]               0\n",
      "           Conv2d-18            [1, 200, 1, 12]         100,200\n",
      "      BatchNorm2d-19            [1, 200, 1, 12]             400\n",
      "              ELU-20            [1, 200, 1, 12]               0\n",
      "        MaxPool2d-21             [1, 200, 1, 6]               0\n",
      "          Dropout-22             [1, 200, 1, 6]               0\n",
      "           Conv2d-23             [1, 152, 1, 2]         152,152\n",
      "      BatchNorm2d-24             [1, 152, 1, 2]             304\n",
      "              ELU-25             [1, 152, 1, 2]               0\n",
      "        MaxPool2d-26             [1, 152, 1, 1]               0\n",
      "          Dropout-27             [1, 152, 1, 1]               0\n",
      "           Conv2d-28             [1, 152, 1, 1]          23,256\n",
      "           Linear-29                     [1, 2]             306\n",
      "================================================================\n",
      "Total params: 313,543\n",
      "Trainable params: 313,543\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.63\n",
      "Params size (MB): 1.20\n",
      "Estimated Total Size (MB): 1.83\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jyt/workspace/fNIRS_models/code_data_tufts/fNIRS-mental-workload-classifiers/helpers/models.py:859: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(out)\n"
     ]
    }
   ],
   "source": [
    "summary(model_2, input_size=(150, 8), batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "flops:  3661274.0 params:  313543.0\n",
      "flops: 3.66 M, params: 0.31 M\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 150, 8).to(device)\n",
    "flops, params = profile(model_2,(dummy_input,))\n",
    "print('flops: ', flops, 'params: ', params)\n",
    "print('flops: %.2f M, params: %.2f M' % (flops / 1000000.0, params / 1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_use_3 = models.DeepConvNet150\n",
    "model_3 = model_to_use_3().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [1, 25, 8, 146]             150\n",
      "            Conv2d-2            [1, 25, 1, 146]           5,000\n",
      "       BatchNorm2d-3            [1, 25, 1, 146]              50\n",
      "               ELU-4            [1, 25, 1, 146]               0\n",
      "         MaxPool2d-5             [1, 25, 1, 73]               0\n",
      "           Dropout-6             [1, 25, 1, 73]               0\n",
      "            Conv2d-7             [1, 50, 1, 69]           6,250\n",
      "       BatchNorm2d-8             [1, 50, 1, 69]             100\n",
      "               ELU-9             [1, 50, 1, 69]               0\n",
      "        MaxPool2d-10             [1, 50, 1, 34]               0\n",
      "          Dropout-11             [1, 50, 1, 34]               0\n",
      "           Conv2d-12            [1, 100, 1, 30]          25,000\n",
      "      BatchNorm2d-13            [1, 100, 1, 30]             200\n",
      "              ELU-14            [1, 100, 1, 30]               0\n",
      "        MaxPool2d-15            [1, 100, 1, 15]               0\n",
      "          Dropout-16            [1, 100, 1, 15]               0\n",
      "           Conv2d-17            [1, 200, 1, 11]         100,000\n",
      "      BatchNorm2d-18            [1, 200, 1, 11]             400\n",
      "              ELU-19            [1, 200, 1, 11]               0\n",
      "        MaxPool2d-20             [1, 200, 1, 5]               0\n",
      "           Conv2d-21               [1, 2, 1, 1]           2,002\n",
      "================================================================\n",
      "Total params: 139,152\n",
      "Trainable params: 139,152\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.59\n",
      "Params size (MB): 0.53\n",
      "Estimated Total Size (MB): 1.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model_3, input_size=(150, 8), batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "torch.Size([1, 2])\n",
      "flops:  3208450.0 params:  139152.0\n",
      "flops: 3.21 M, params: 0.14 M\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 150, 8).to(device)\n",
    "flops, params = profile(model_3,(dummy_input,))\n",
    "print('flops: ', flops, 'params: ', params)\n",
    "print('flops: %.2f M, params: %.2f M' % (flops / 1000000.0, params / 1000000.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fNIRS-mental-workload-classifiers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d93f6ccf2d4c808a6e5ed058fec4d677fc443ecf11a44bde689b875892dca5c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
